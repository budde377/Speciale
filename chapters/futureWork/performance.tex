\newpage
\section{Performance}
\label{sec:future3}
Performance has not been a primary concern of this thesis, and as such only few thoughts have been put into optimization of the performance. The analysis in practice has high memory consumption and while it might terminate within a few seconds, even that might be too long for it to be used as a tool in a development environment where the changes are frequent.  

When optimizing for performance, two approaches can be considered; optimizing theoretical performance and optimizing practical performance. The former is the performance of the specified analysis, whereas the latter is optimization in the implementation, while still conforming to the specification of the analysis.

\subsection{Analysis performance}
The analysis lattice is the primary data-structure used and thus a major factor in performance regarding memory usage. To obtain context sensitivity the State-lattice is stored multiple times in the analysis lattice and because of this minimizing the size of the State-lattice potentially results in multiple times the performance gain.

The two lattice-parts Temps and HeapTemps are used to pass information between control-flow nodes and are specified as complete maps from temporary variable names and temporary heap variable names, respectively. Consider $\bot$-values smaller to store than other values as this is the default value. The current analysis never resets entries from the map, which implies that it will keep growing in size with programs of larger size, as more temporary storage entries are set to values different than $\bot$. Due to the nature of how these maps are used, each entry can be seen as having a provider and potentially also a consumer control-flow node. The provider node sets the entry which is later used by the consumer node. To keep the maps small in size, even with large programs, the consumer nodes can reset the entries they consume to $\bot$. Graph \ref{graph:exexpr1} shows a simple control-flow graph to demonstrate the effect of letting the consumer node reset consumed entries.
\begin{graph}
\centering
\begin{adjustbox}{max size={1\textwidth}{.25\textheight}, margin = 10pt}\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
	\node [entry] (en) {};
 	\node [node, right of=en, label={270:$\mathit{constRead}(\texttt{1},t_1)$}] (c1) {};
 	\node [node, right of=c1, label={90:$\mathit{constRead}(\texttt{2},t_3)$}] (c2) {};
 	\node [node, right of=c2, label={270:$\mathit{constRead}(\texttt{3},t_4)$}] (c3) {};
 	\node [node, right of=c3, label={90:$\mathit{bop}_+(t_3,t_4, t_2)$}] (bop1) {};
 	\node [node, right of=bop1, label={270:$\mathit{bop}_+(t_1,t_2, t)$}] (bop2) {};
	\node [exit, right of=bop2] (ex) {};
    % Draw edges
    \path[line] (en) -> (c1);
    \path[line] (c1) -> (c2);
    \path[line] (c2) -> (c3);
    \path[line] (c3) -> (bop1);
     \path[line] (bop1) -> (bop2);
    \path[line] (bop2) -> (ex);

\end{tikzpicture}\end{adjustbox}

\caption{Example graph $\subt{\texttt{1+(2+3)}}(t)$}
\label{graph:exexpr1}
\end{graph}

In the program below, the first line shows the entries which would be set, i.e.\ not $\bot$, in the Temps lattice-element associated with the last node without any optimizations. The second line shows the set entries with the optimization.
\begin{lstlisting}[mathescape]
temps $\rightarrow$ [t1, t2, t3, t4, t] // without optimization
temps $\rightarrow$ [t] // with optimization
\end{lstlisting}

\subsection{Implementation performance}
The implemented analysis cache results and performs joins lazily, which should in theory limit the computation necessary. There is however much more to be done, and it is expected that existing techniques could be used for identifying possible optimizations. For example, it could be considered how running the analysis on the whole program even for the smallest change could be avoided by reusing abstract states from unchanged code. 

%\todo{write more}